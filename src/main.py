"""
–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å Educational Plagiarism Detector.
"""

import os
import json
import argparse
import datetime
import glob

from .utils import (
    preprocess_text,
    tokenize_and_lemmatize,
    create_similarity_matrix,
    read_text_file,
    extract_text_from_pdf
)


def detect_plagiarism_in_directory(directory: str) -> dict:
    """
    –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–ª–∞–≥–∏–∞—Ç–∞ –≤–æ –≤—Å–µ—Ö —Ñ–∞–π–ª–∞—Ö —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    
    Args:
        directory: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ñ–∞–π–ª–∞–º–∏
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {directory}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤
    supported_extensions = ['.txt', '.pdf', '.docx']
    files = []
    
    for ext in supported_extensions:
        pattern = os.path.join(directory, f'*{ext}')
        files.extend(glob.glob(pattern))
    
    if len(files) < 2:
        return {
            'status': 'error',
            'message': f'Need at least 2 files for comparison. Found: {len(files)}',
            'timestamp': datetime.datetime.now().isoformat()
        }
    
    print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
    
    # –ß—Ç–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤
    texts = []
    filenames = []
    errors = []
    
    for filepath in files:
        filename = os.path.basename(filepath)
        print(f"  üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {filename}")
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞ –∏ —á—Ç–µ–Ω–∏–µ
            if filepath.endswith('.txt'):
                text = read_text_file(filepath)
            elif filepath.endswith('.pdf'):
                text = extract_text_from_pdf(filepath)
            else:
                errors.append(f"Unsupported file type: {filename}")
                continue
            
            if not text.strip():
                errors.append(f"Empty file: {filename}")
                continue
            
            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            cleaned_text = preprocess_text(text)
            tokens = tokenize_and_lemmatize(cleaned_text)
            processed_text = ' '.join(tokens)
            
            texts.append(processed_text)
            filenames.append(filename)
            
            print(f"    ‚úÖ –£—Å–ø–µ—à–Ω–æ ({len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤)")
            
        except Exception as e:
            error_msg = f"Error processing {filename}: {str(e)}"
            errors.append(error_msg)
            print(f"    ‚ùå {error_msg}")
    
    if len(texts) < 2:
        return {
            'status': 'error',
            'message': 'Less than 2 valid files after processing',
            'errors': errors,
            'timestamp': datetime.datetime.now().isoformat()
        }
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
    print("\nüî¨ –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏...")
    similarity_results = create_similarity_matrix(texts)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    result = {
        'status': 'success',
        'timestamp': datetime.datetime.now().isoformat(),
        'total_files': len(files),
        'processed_files': len(texts),
        'filenames': filenames,
        'errors': errors,
        'similarity_matrices': {
            method: matrix.tolist()
            for method, matrix in similarity_results.items()
        }
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_dir = 'results'
    os.makedirs(results_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = os.path.join(results_dir, f'plagiarism_results_{timestamp}.json')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_file}")
    
    # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏
    print("\nüìä –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    print("=" * 60)
    
    combined_matrix = similarity_results['combined']
    for i in range(len(filenames)):
        for j in range(i + 1, len(filenames)):
            similarity = combined_matrix[i][j]
            
            if similarity > 0.7:
                status = "üö® –í–´–°–û–ö–ê–Ø –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ –ü–õ–ê–ì–ò–ê–¢–ê"
                emoji = "‚ö†Ô∏è"
            elif similarity > 0.4:
                status = "‚ö†Ô∏è  –£–ú–ï–†–ï–ù–ù–ê–Ø –°–•–û–ñ–ï–°–¢–¨"
                emoji = "üîç"
            else:
                status = "‚úÖ –ù–ò–ó–ö–ê–Ø –°–•–û–ñ–ï–°–¢–¨"
                emoji = "‚úì"
            
            print(f"{emoji} {filenames[i]} vs {filenames[j]}: {similarity:.2%} - {status}")
    
    print("=" * 60)
    
    return result


def analyze_single_pair(file1: str, file2: str) -> dict:
    """
    –ê–Ω–∞–ª–∏–∑ —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è —Ñ–∞–π–ª–∞–º–∏.
    
    Args:
        file1: –ü—É—Ç—å –∫ –ø–µ—Ä–≤–æ–º—É —Ñ–∞–π–ª—É
        file2: –ü—É—Ç—å –∫–æ –≤—Ç–æ—Ä–æ–º—É —Ñ–∞–π–ª—É
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    print("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤:")
    print(f"   1. {os.path.basename(file1)}")
    print(f"   2. {os.path.basename(file2)}")
    
    # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    texts = []
    filenames = []
    
    for filepath in [file1, file2]:
        filename = os.path.basename(filepath)
        
        try:
            if filepath.endswith('.txt'):
                text = read_text_file(filepath)
            elif filepath.endswith('.pdf'):
                text = extract_text_from_pdf(filepath)
            else:
                raise ValueError(f"Unsupported file type: {filepath}")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = preprocess_text(text)
            tokens = tokenize_and_lemmatize(cleaned_text)
            processed_text = ' '.join(tokens)
            
            texts.append(processed_text)
            filenames.append(filename)
            
            print(f"   ‚úÖ {filename}: {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤")
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Error processing {filename}: {str(e)}',
                'timestamp': datetime.datetime.now().isoformat()
            }
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
    from .utils import (
        calculate_cosine_similarity,
        calculate_lcs_similarity,
        calculate_ngram_similarity
    )
    
    # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
    cosine_sim = calculate_cosine_similarity(texts)[0][1]
    
    # LCS —Å—Ö–æ–∂–µ—Å—Ç—å
    lcs_sim = calculate_lcs_similarity(texts[0], texts[1])
    
    # N-gram —Å—Ö–æ–∂–µ—Å—Ç—å
    ngram_sim = calculate_ngram_similarity(texts[0], texts[1])
    
    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
    combined_sim = 0.5 * cosine_sim + 0.3 * lcs_sim + 0.2 * ngram_sim
    
    result = {
        'status': 'success',
        'timestamp': datetime.datetime.now().isoformat(),
        'files': filenames,
        'similarity_scores': {
            'cosine': cosine_sim,
            'lcs': lcs_sim,
            'ngram': ngram_sim,
            'combined': combined_sim
        },
        'interpretation': interpret_similarity(combined_sim)
    }
    
    print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   Cosine Similarity: {cosine_sim:.2%}")
    print(f"   LCS Similarity: {lcs_sim:.2%}")
    print(f"   N-gram Similarity: {ngram_sim:.2%}")
    print(f"   Combined Similarity: {combined_sim:.2%}")
    print(f"   –í–µ—Ä–¥–∏–∫—Ç: {result['interpretation']}")
    
    return result


def interpret_similarity(score: float) -> str:
    """
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏.
    
    Args:
        score: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
        
    Returns:
        –¢–µ–∫—Å—Ç–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
    """
    if score > 0.7:
        return "üö® –í–´–°–û–ö–ê–Ø –í–ï–†–û–Ø–¢–ù–û–°–¢–¨ –ü–õ–ê–ì–ò–ê–¢–ê - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è"
    elif score > 0.4:
        return "‚ö†Ô∏è  –£–ú–ï–†–ï–ù–ù–ê–Ø –°–•–û–ñ–ï–°–¢–¨ - –≤–æ–∑–º–æ–∂–Ω—ã –∑–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–∏—è"
    elif score > 0.2:
        return "üîç –ù–ï–ë–û–õ–¨–®–ê–Ø –°–•–û–ñ–ï–°–¢–¨ - –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"
    else:
        return "‚úÖ –ù–ò–ó–ö–ê–Ø –°–•–û–ñ–ï–°–¢–¨ - –ø–ª–∞–≥–∏–∞—Ç –º–∞–ª–æ–≤–µ—Ä–æ—è—Ç–µ–Ω"


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description='Educational Plagiarism Detector - —Å–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–ª–∞–≥–∏–∞—Ç–∞ –≤ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö'
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    dir_parser = subparsers.add_parser('analyze-dir', help='Analyze all files in a directory')
    dir_parser.add_argument('directory', help='Directory with student files')
    
    # –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤—É—Ö —Ñ–∞–π–ª–æ–≤
    pair_parser = subparsers.add_parser('compare', help='Compare two specific files')
    pair_parser.add_argument('file1', help='First file')
    pair_parser.add_argument('file2', help='Second file')
    
    args = parser.parse_args()
    
    if args.command == 'analyze-dir':
        detect_plagiarism_in_directory(args.directory)
    elif args.command == 'compare':
        analyze_single_pair(args.file1, args.file2)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()